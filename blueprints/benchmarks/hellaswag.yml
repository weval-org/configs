title: "What the HellaSwag? On the Validity of Common-Sense Reasoning Benchmarks"
description: |
  HellaSwag is a widely used benchmark designed to evaluate common-sense reasoning in language models. It aims to measure a model's general language and world understanding by presenting a situation and requiring the model to choose the most plausible sentence completion from multiple options. The benchmark was created by generating alternative completions to segments from ActivityNet and WikiHow datasets, followed by adversarial filtering and human validation.

  However, this paper argues that HellaSwag suffers from severe construct validity issues, including:
  - Ungrammaticality and typos in prompts and answer options.
  - Nonsensical constructions in prompts and correct/incorrect answers.
  - Lack of distinct correct answers, with multiple equally plausible options or no plausible options at all.
  - Correlation between answer likelihood and length.
  - The question text often not being essential for model predictions, as shown by zero-prompt evaluations.

  These issues suggest that HellaSwag, in its current state, does not accurately measure common-sense reasoning. The paper proposes requirements for future common-sense reasoning benchmarks and introduces GoldenSwag, a highly filtered subset of HellaSwag, to address these validity concerns.
author:
  name: Pavel Chizhov, Mattia Nee, Pierre-Carl Langlais, Ivan P. Yamshchikov
references:
  - title: "What the HellaSwag? On the Validity of Common-Sense Reasoning Benchmarks"
    url: https://arxiv.org/abs/2504.07825
tags:
  - common-sense reasoning
  - natural language understanding
  - multiple-choice
  - benchmark evaluation
  - validity
  - grammar
  - coherence
  - typos
  - bias
  - zero-prompt
models:
  - CORE
---
- id: hellaswag-example-1
  prompt: "Context: Clean and jerk: Women are in the background of a gym lifting weights. Man\n1. is preparing himself to lift weigh and stands in front of weight.\n2. is boxing with woman in an arena.\n3. is running in a marathon in a large arena and people is standing around.\n4. is standing in front of a woman doing weight lifting with camera around a table."
  ideal: "1"
  should:
    - "The model selects the most plausible completion of the context."
    - $icontains: "1"
    - "The chosen option is grammatically correct and sensical."
  should_not:
    - "The response contains typos or ungrammatical phrasing."
    - "The response selects an option that is nonsensical or incoherent."
- id: generation-prompt-example-1
  prompt: "Context: Roof shingle removal: A man is sitting on a roof. He\n1. is using wrap to wrap a pair of skis.\n2. is ripping level tiles off.\n3. is holding a Rubik's cube.\n4. starts pulling up roofing on a roof."
  system: "You are given a situation followed by four possible endings. Choose the most appropriate ending by selecting the corresponding number. Respond only with the number of the correct answer."
  ideal: "4"
  should:
    - $matches: "^[1-4]$"
    - $icontains: "4"
    - "The chosen option is the most plausible completion of the context."
    - "The chosen option is grammatically correct and sensical."
  should_not:
    - "The response contains any text other than the single digit answer."
    - "The response selects an option that is clearly less plausible than the correct one."
- id: annotation-example-triple-jump
  prompt: "A) Triple jump: As he reaches the dirt section, he does three jumps. On his final jump, he extends his legs to try and jump as far as possible. The video ends with closing captions.\nB) Triple jump: As he reaches the dirt section, he does three jumps. On his final jump, he extends his legs to try and jump as far as possible. The video ends after he lands and he is shown smacking the ground. (Labeled as correct)\nC) Triple jump: As he reaches the dirt section, he does three jumps. On his final jump, he extends his legs to try and jump as far as possible. The video ends with the intro credits shown.\nD) Triple jump: As he reaches the dirt section, he does three jumps. On his final jump, he extends his legs to try and jump as far as possible. The video ends with more dirt."
  ideal: "B"
  should:
    - "The model selects the option labeled as correct."
    - $icontains: "B"
    - "The chosen option is plausible and sensical."
  should_not:
    - "The model selects an option that is equally plausible to the correct one but is not labeled as correct."
    - "The model selects the worst answer choice."
- id: annotation-example-cheerleading
  prompt: "A) Cheerleading: A cheerleading team begins to hold up posters as their mascot runs behind them. They eventually get the dogs on the field and push them around.\nB) Cheerleading: A cheerleading team begins to hold up posters as their mascot runs behind them. They begin to perform an athlete's routine on the field.\nC) Cheerleading: A cheerleading team begins to hold up posters as their mascot runs behind them. They then begin to do a routine, and some of the girls run with streamers as the rest hold up the girls for their stunt. (Labeled as correct)\nD) Cheerleading: A cheerleading team begins to hold up posters as their mascot runs behind them. They begin to perform acoustic songs."
  ideal: "C"
  should:
    - "The model selects the option labeled as correct."
    - $icontains: "C"
    - "The chosen option is plausible and sensical."
  should_not:
    - "The model selects an option that is equally plausible to the correct one but is not labeled as correct."
    - "The model selects the worst answer choice."