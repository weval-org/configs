title: "JETTS Benchmark: Evaluating LLM-as-Judges as Test-Time Scaling Evaluators"
description: |
  The Judge Evaluation for Test-Time Scaling (JETTS) benchmark systematically evaluates the effectiveness of LLM-as-judges in test-time scaling scenarios. Unlike traditional benchmarks that use fixed pairwise test sets, JETTS simulates three distinct test-time compute tasks:
  
  *   **Response Reranking:** Judges select the best response from a set of generated candidates.
  *   **Step-Level Beam Search:** Judges guide a generator by evaluating partial responses during a beam search process.
  *   **Critique-Based Response Refinement:** Judges provide natural language critiques and ratings to iteratively refine a generator's response.
  
  The benchmark measures the improvement LLM judges bring to generator models across three domains: math reasoning, code generation, and instruction following. It aims to understand if judges can effectively guide generators towards better responses, especially in complex, reasoning-intensive tasks, and to assess the utility of their natural language critiques.

  Source: This benchmark is described in the paper "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators" (arXiv:2504.15253).
author:
  name: "Yilun Zhou, Austin Xu, Peifeng Wang, Caiming Xiong, Shafiq Joty"
  url: "https://arxiv.org/abs/2504.15253"
references:
  - title: "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators"
    url: "https://arxiv.org/abs/2504.15253"
tags:
  - reasoning
  - code
  - math
  - instruction-following
  - ranking
  - generation
  - evaluation
  - llm-as-judge
models:
  - CORE
---
- id: "pairwise-general-instruction-following"
  prompt: |
    Your reply should strictly follow this format:
    **Reasoning:** <feedback evaluating the responses>
    **Result:** <A or B>
    Here is the data.
    Instruction:
    """
    {query_text}
    """
    Response A:
    """
    {response_A}
    """
    Response B:
    """
    {response_B}
    """
  system: "You are a helpful assistant in evaluating the quality of the responses for a given instruction. Your goal is to select the best response for the given instruction. Select Response A or Response B, that is better for the given instruction. The two responses are generated by two different AI chatbots respectively. Do NOT say both / neither are good."
  should:
    - "$matches: \"^\\*\\*Reasoning:\\*\\* .*\\n\\*\\*Result:\\*\\* (A|B)$\""
    - "Prioritizes evaluating whether the output honestly, precisely, and closely executes the instruction."
    - "Considers helpfulness, accuracy, level of detail, and harmlessness."
    - "Judgment is objective and avoids potential bias."
    - "The order of responses (A or B) does not affect judgment."
    - "The length of responses does not affect judgment, and the response length is appropriate for the given instruction."
  should_not:
    - "The response contains more or less than what the instruction asks for."
- id: "pairwise-math-reasoning-beam-search"
  prompt: |
    Important note: the model response is generated with streaming, meaning that it may not be finished yet but we still want to get a sense of the partial progress so far. If this is the case, you should provide judgment and feedback on the currently generated part, focusing on its correctness and promise of leading to a correct final solution.

    Your reply should strictly follow this format:
    **Reasoning:** <feedback evaluating the responses>
    **Result:** <A or B>
    Here is the data.
    Instruction:
    """
    {query_text}
    """
    Response A:
    """
    {response_A}
    """
    Response B:
    """
    {response_B}
    """
  system: "You are a helpful assistant in evaluating the quality of the responses for a given instruction, which is a math problem. Your goal is to select the best response for the given instruction. Select Response A or Response B, that is better for the given math problem. The two responses are generated by two different AI chatbots respectively. Do NOT say both / neither are good."
  should:
    - "$matches: \"^\\*\\*Reasoning:\\*\\* .*\\n\\*\\*Result:\\*\\* (A|B)$\""
    - "Prioritizes evaluating if the output arrives at the correct solution for the given math problem."
    - "The logical reasoning to derive the solution is sound."
    - "If both responses arrive at the correct solution, chooses the response that contains the better logical reasoning, prioritizing correctness, then completeness and conciseness."
    - "Judgment is objective and avoids potential bias."
    - "The order of responses (A or B) does not affect judgment."
    - "The length of responses does not affect judgment, and the response length is appropriate for the given instruction."
  should_not:
    - "The response contains more or less than what the question asks for."
- id: "pairwise-code-generation-beam-search"
  prompt: |
    Important note: the model response is generated with streaming, meaning that it may not be finished yet but we still want to get a sense of the partial progress so far. If this is the case, you should provide judgment and feedback on the currently generated part, focusing on its correctness and promise of leading to a correct final solution.

    Your reply should strictly follow this format:
    **Reasoning:** <feedback evaluating the responses>
    **Result:** <A or B>
    Here is the data.
    Instruction:
    """
    {query_text}
    """
    Response A:
    """
    {response_A}
    """
    Response B:
    """
    {response_B}
    """
  system: "You are a helpful assistant in evaluating the quality of the responses for a given instruction, which is a coding problem. Your goal is to select the best response for the given instruction. Select Response A or Response B, that is better for the given coding problem. The two responses are generated by two different AI chatbots respectively. Do NOT say both / neither are good."
  should:
    - "$matches: \"^\\*\\*Reasoning:\\*\\* .*\\n\\*\\*Result:\\*\\* (A|B)$\""
    - "Prioritizes evaluating if the output response code correctly implements the desired functionality in the instruction."
    - "If both responses correctly implement the desired functionality, chooses the response that contains the better written code, prioritizing conciseness and readability."
    - "Judgment is objective and avoids potential bias."
    - "The order of responses (A or B) does not affect judgment."
    - "The length of responses does not affect judgment, and the response length is appropriate for the given instruction."
  should_not:
    - "The response contains more or less than what the question asks for."
- id: "likert-single-rating-general"
  prompt: |
    Your reply should strictly follow this format:
    **Reasoning:** <Your feedback>
    **Result:** <an integer between 1 and 5>
    Here is the data.
    Instruction:
    """
    {query_text}
    """
    Response:
    """
    {response}
    """
  system: "You are tasked with evaluating a response based on a given instruction (which may contain an Input). Provide a comprehensive feedback on the response quality based on the rules for evaluation. Follow this with a score between 1 and 5. Avoid generating any additional opening, closing, or explanations."
  should:
    - "$matches: \"^\\*\\*Reasoning:\\*\\* .*\\n\\*\\*Result:\\*\\* ([1-5])$\""
    - "Prioritizes evaluating whether the output honestly, precisely, and closely executes the instruction."
    - "Considers helpfulness, accuracy, level of detail, and harmlessness."
    - "Judgment is objective and avoids potential bias."
    - "The length of the response does not affect judgment, and the response length is appropriate for the given instruction."
  should_not:
    - "The response contains more or less than what the instruction asks for."
- id: "additive-single-rating-general"
  prompt: |
    Your reply should strictly follow this format:
    **Reasoning:** <Your feedback>
    **Result:** <an integer between 1 and 5>
    Here is the data.
    Instruction:
    """
    {query_text}
    """
    Response:
    """
    {response}
    """
    Score rubrics:
    - Add one point if the response is relevant and provides some information related to the user's inquiry, even if it is incomplete or contains some irrelevant content.
    - Add a second point if the response addresses a substantial portion of the user's question, but does not completely resolve the query or provide a direct answer.
    - Add a third point if the response answers the basic elements of the user's question in a useful way, regardless of whether it seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results.
    - Add a fourth point if the response is clearly written from an AI Assistant's perspective, addressing the user's question directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity, conciseness or focus.
    - Add a fifth point for a response that is impeccably tailored to the user's question by an AI Assistant, without extraneous information, reflecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer.
  system: "You are tasked with evaluating a response based on a given instruction (which may contain an Input). Provide a comprehensive feedback on the response quality based on the rules for evaluation. Follow this with a score between 1 and 5. Avoid generating any additional opening, closing, or explanations."
  should:
    - "$matches: \"^\\*\\*Reasoning:\\*\\* .*\\n\\*\\*Result:\\*\\* ([1-5])$\""
    - "Prioritizes evaluating whether the response satisfies the provided rubric."
    - "The basis of the score depends exactly on the rubric."
    - "Judgment is objective and avoids potential bias."
    - "The length of the response does not affect judgment, and the response length is appropriate for the given instruction."
    - "Score rubrics: Add one point if the response is relevant and provides some information related to the user's inquiry, even if it is incomplete or contains some irrelevant content."
    - "Score rubrics: Add a second point if the response addresses a substantial portion of the user's question, but does not completely resolve the query or provide a direct answer."
    - "Score rubrics: Add a third point if the response answers the basic elements of the user's question in a useful way, regardless of whether it seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results."
    - "Score rubrics: Add a fourth point if the response is clearly written from an AI Assistant's perspective, addressing the user's question directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity, conciseness or focus."
    - "Score rubrics: Add a fifth point for a response that is impeccably tailored to the user's question by an AI Assistant, without extraneous information, reflecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer."
- id: "generator-critique-based-refinement"
  prompt: |
    <query>
    {query}
    </query>
    <original_response>
    {response}
    </original_response>
    <score>
    {score} out of {max_score}.
    </score>
    <explanation>
    {explanation}
    </explanation>
    Your revision (or "No revision needed."):
  system: "You help revise a machine-generated response to a user query. Below, you will be provided with the user query and the machine-generated response. You will also be provided with the output of an evaluator model, which gives a score (max being {max_score}) and an explanation for the score. You should revise and improve the current response, following the evaluator's recommendation. If the evaluator does not identify any area of improvement, you should output \"No revision needed.\" Otherwise, you should output the revised response surrounded by the <revised_response> and </revised_response> tags. You do not need to output anything else."
  should:
    - "$matches: \"^(No revision needed\\.|<revised_response>.*</revised_response>)$\""
    - "Revises and improves the current response, following the evaluator's recommendation."
    - "If the evaluator does not identify any area of improvement, outputs \"No revision needed.\""
    - "Otherwise, outputs the revised response surrounded by the <revised_response> and </revised_response> tags."