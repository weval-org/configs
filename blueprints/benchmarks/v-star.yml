title: "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning"
description: |
  V-STaR is a benchmark designed to evaluate Video Large Language Models (Video-LLMs) on their video spatio-temporal reasoning abilities. It addresses the shortcomings of existing benchmarks that primarily focus on object presence and neglect relational reasoning.

  The core methodology involves decomposing video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task. This task simultaneously evaluates 'what' objects are present, 'when' events occur, and 'where' they are located, while capturing the underlying Chain-of-Thought (CoT) logic. The benchmark uses a fine-grained reasoning dataset with coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. It also features two RSTR task chains ('what-when-where' and 'what-where-when') to quantify the influence of different logical sequences on reasoning.

  The evaluation pipeline uses an 'Instruction Prompt' for GPT-4-turbo to generate reasoning chains for VQA, focusing on sequential temporal and spatial questions. To mimic human cognition, explicit reasoning chains are embedded within custom `<think><think>` tags for each question. The subsequent questions in a reasoning chain incorporate the ground truth of the previous question to prevent error propagation.

  Responses are scored based on the type of question:
  1.  **'What' questions (VQA)**: Evaluated using Accuracy (Acc). Answers are scored from 0 to 4 by Qwen2.5-72B-Instruct, where 0 is 'entirely incorrect' and 4 is 'entirely correct'. Answers scoring above 2 are considered correct.
  2.  **'When' questions (Temporal Grounding)**: Evaluated using Mean Temporal Intersection over Union (m_tIoU) and R@n, tIoU=m.
  3.  **'Where' questions (Spatial Grounding)**: Evaluated using Mean Visual Intersection over Union (m_vIoU) and Average Precision score (AP@vIoU=m).

  For overall performance, aggregate metrics like Arithmetic Mean (AM) and Logarithmic Geometric Mean (LGM) are used across 'what', 'when', and 'where' components. Mean AM (mAM) and Mean LGM (mLGM) assess performance across different reasoning chains.

  Source: [V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning](https://arxiv.org/abs/2503.11495)
author:
  name: "Zixu Cheng, Jian Hu, Ziquan Liu, Chenyang Si, Wei Li, Shaogang Gong"
references:
  - title: "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning"
    url: "https://arxiv.org/abs/2503.11495"
tags:
  - reasoning
  - video understanding
  - spatio-temporal
  - grounding
  - chain-of-thought
  - VQA
models:
  - CORE
---
- id: fig1-what-1
  prompt: "Who holds the cellphone?"
  ideal: "A boy holds the cellphone."
  should:
    - "Correctly identifies the object(s) or entity described in the video."
- id: fig1-when-1
  prompt: "When is the moment 'a boy holds a cellphone'?"
  ideal: "[24.3s, 49.6s]"
  should:
    - "$matches: \"^\\[\\\\d+\\\\.\\\\d+s, \\\\d+\\\\.\\\\d+s\\]$\""
    - "The provided temporal range accurately corresponds to the event described."
- id: fig1-where-1
  prompt: "Where is the boy holding a cellphone?"
  ideal: "\"24\": [90,0,373,359],..., \"49\": [102,0,506,356]"
  should:
    - "$matches: \".*\\\"\\\\d+\\\": \\[\\\\d+,\\\\d+,\\\\d+,\\\\d+\\].*\""
    - "The provided bounding box information accurately localizes the object(s) in the video."
- id: fig1-what-2
  prompt: "Who does the man hold?"
  ideal: "A child."
  should:
    - "Correctly identifies the object(s) or entity described in the video."
- id: fig1-when-2
  prompt: "When is the moment 'a man holds a child'?"
  ideal: "[3.8s, 12.7s]"
  should:
    - "$matches: \"^\\[\\\\d+\\\\.\\\\d+s, \\\\d+\\\\.\\\\d+s\\]$\""
    - "The provided temporal range accurately corresponds to the event described."
- id: fig1-where-2
  prompt: "Where is the child the man hold?"
  ideal: "\"4\": [394,20,509,106], ..., \"12\": [135,11,352,342]"
  should:
    - "$matches: \".*\\\"\\\\d+\\\": \\[\\\\d+,\\\\d+,\\\\d+,\\\\d+\\].*\""
    - "The provided bounding box information accurately localizes the object(s) in the video."
- id: fig4-main-q
  prompt: "What does Leonard look at when he talks to his friends?"
  ideal: "His laptop."
  should:
    - "Correctly identifies the object(s) or entity described in the video."
- id: fig4-rstr1-when
  prompt: "When does Leonard look at his laptop while talking to his friends?"
  ideal: "timestamps: [0, 7.83]"
  should:
    - "$matches: \"^timestamps: \\[\\\\d+, \\\\d+\\\\.\\\\d+\\]$\""
    - "The provided temporal range accurately corresponds to the event described."
- id: fig4-rstr1-where
  prompt: "Where is the laptop that Leonard looks at while talking to his friends?"
  ideal: "bboxes: {0:[142,221,421,360], 2: [335,192,446,280]..."
  should:
    - "$matches: \"^bboxes: \\{\\\\d+:\\[\\\\d+,\\\\d+,\\\\d+,\\\\d+\\].*\""
    - "The provided bounding box information accurately localizes the object(s) in the video."
- id: fig4-rstr2-where
  prompt: "Where is the laptop that Leonard looks at while talking to his friends during the time {timestamps}?"
  ideal: "bboxes: {0:[142,221,421,360], 2: [335,192,446,280]..."
  should:
    - "$matches: \"^bboxes: \\{\\\\d+:\\[\\\\d+,\\\\d+,\\\\d+,\\\\d+\\].*\""
    - "The provided bounding box information accurately localizes the object(s) in the video."
- id: fig4-rstr2-when
  prompt: "With the bounding box without timestamps provided {bboxes}, When does Leonard look at his laptop while talking to his friends?"
  ideal: "timestamps: [0, 7.83]"
  should:
    - "$matches: \"^timestamps: \\[\\\\d+, \\\\d+\\\\.\\\\d+\\]$\""
    - "The provided temporal range accurately corresponds to the event described."
- id: fig6-main-q
  prompt: "Who holds the bat on the grass?"
  ideal: "An adult man."
  should:
    - "Correctly identifies the object(s) or entity described in the video."
- id: fig6-rstr1-when
  prompt: "When is the moment 'an adult holds a bat on the grass'?"
  ideal: "[0.0s, 9.1s]"
  should:
    - "$matches: \"^\\[\\\\d+\\\\.\\\\d+s, \\\\d+\\\\.\\\\d+s\\]$\""
    - "The provided temporal range accurately corresponds to the event described."
- id: fig6-rstr1-where
  prompt: "Where is the adult holding a bat on the grass between 0s~9.1s?"
  ideal: "GT: \"0\":[152,93,414,438], ..., \"9\":[151,93,413,435]"
  should:
    - "$matches: \"^GT: \\\"\\\\d+\\\":\\[\\\\d+,\\\\d+,\\\\d+,\\\\d+\\].*\""
    - "The provided bounding box information accurately localizes the object(s) in the video."
- id: fig6-rstr2-where
  prompt: "Where is the adult holding a bat on the grass?"
  ideal: "GT: \"0\":[152,93,414,438], ..., \"9\":[151,93,413,435]"
  should:
    - "$matches: \"^GT: \\\"\\\\d+\\\":\\[\\\\d+,\\\\d+,\\\\d+,\\\\d+\\].*\""
    - "The provided bounding box information accurately localizes the object(s) in the video."
- id: fig6-rstr2-when
  prompt: "With the bbox...provided, when is the moment 'an adult holds a bat on the grass'?"
  ideal: "[0.0s, 9.1s]"
  should:
    - "$matches: \"^\\[\\\\d+\\\\.\\\\d+s, \\\\d+\\\\.\\\\d+s\\]$\""
    - "The provided temporal range accurately corresponds to the event described."